{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kZdldB8gduL"
      },
      "source": [
        "# E-mail Spam Filtering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owetTehhuuQ8"
      },
      "source": [
        "importing the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_51LUWzOaieJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT3ilJ-Xuy6k"
      },
      "source": [
        "# 1) Data Preparation\n",
        "downloading and extracting the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCsqOftgckZq",
        "outputId": "71c5da46-8cb8-4da5-b995-0dddf0ba00c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-11-01 00:41:21--  http://www.aueb.gr/users/ion/data/lingspam_public.tar.gz\n",
            "Resolving www.aueb.gr (www.aueb.gr)... 195.251.255.156\n",
            "Connecting to www.aueb.gr (www.aueb.gr)|195.251.255.156|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://pages.aueb.gr/users/ion/data/lingspam_public.tar.gz [following]\n",
            "--2023-11-01 00:41:21--  http://pages.aueb.gr/users/ion/data/lingspam_public.tar.gz\n",
            "Resolving pages.aueb.gr (pages.aueb.gr)... 195.251.255.230\n",
            "Connecting to pages.aueb.gr (pages.aueb.gr)|195.251.255.230|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www2.aueb.gr/users/ion/data/lingspam_public.tar.gz [following]\n",
            "--2023-11-01 00:41:22--  https://www2.aueb.gr/users/ion/data/lingspam_public.tar.gz\n",
            "Resolving www2.aueb.gr (www2.aueb.gr)... 195.251.255.230\n",
            "Connecting to www2.aueb.gr (www2.aueb.gr)|195.251.255.230|:443... connected.\n",
            "WARNING: cannot verify www2.aueb.gr's certificate, issued by ‘CN=GEANT OV RSA CA 4,O=GEANT Vereniging,C=NL’:\n",
            "  Unable to locally verify the issuer's authority.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11564714 (11M) [application/x-gzip]\n",
            "Saving to: ‘lingspam_public.tar.gz’\n",
            "\n",
            "lingspam_public.tar 100%[===================>]  11.03M  4.92MB/s    in 2.2s    \n",
            "\n",
            "2023-11-01 00:41:25 (4.92 MB/s) - ‘lingspam_public.tar.gz’ saved [11564714/11564714]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://www.aueb.gr/users/ion/data/lingspam_public.tar.gz --no-check-certificate\n",
        "!tar -xzf lingspam_public.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNSAw0Otu4TF"
      },
      "source": [
        "preparing the data. extracting the emails and labels from the files in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0XejCLkjLJh"
      },
      "outputs": [],
      "source": [
        "# Path to the parent directory\n",
        "parent_dir = \"lingspam_public/lemm_stop\"\n",
        "\n",
        "# Lists to store training and testing emails\n",
        "train_emails = []\n",
        "train_labels = []\n",
        "test_emails = []\n",
        "test_labels = []\n",
        "\n",
        "# iterate over each sub-folder\n",
        "for folder in os.listdir(parent_dir):\n",
        "   # construct the full path to the sub-folder\n",
        "   folder_path = os.path.join(parent_dir, folder)\n",
        "\n",
        "   # check if the path is a directory\n",
        "   if os.path.isdir(folder_path):\n",
        "       # tterate over each file in the sub-folder\n",
        "       for filename in os.listdir(folder_path):\n",
        "           # construct the full path to the file\n",
        "           file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "           # check if the path is a file\n",
        "           if os.path.isfile(file_path):\n",
        "               # open the file and read the emails\n",
        "               with open(file_path, \"r\") as f:\n",
        "                  emails = f.readlines()\n",
        "                  # if the filename starts with 'spmsg', it is considered spam, otherwise it's not\n",
        "                  is_spam = [1 if filename.startswith('spmsg') else 0 for _ in emails]\n",
        "                  # if the folder is \"part10\", use it for testing, otherwise use it for training\n",
        "                  if folder == \"part10\":\n",
        "                      test_emails.extend(emails)\n",
        "                      test_labels.extend(is_spam)\n",
        "                  else:\n",
        "                      train_emails.extend(emails)\n",
        "                      train_labels.extend(is_spam)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbkP1RwZvZfg"
      },
      "source": [
        " # 2) Feature selection using the information gain\n",
        " feature selection using the information gain (IG) metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHF91uZX3oNR",
        "outputId": "3373fb1f-d9ca-46ac-bd16-ee851d8dd796"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 features: ['business', 'click', 'free', 'language', 'linguistic', 'market', 'money', 'our', 'remove', 'university']\n",
            "Top 100 features: ['100', '20', 'abstract', 'ad', 'address', 'advertise', 'amaze', 'anywhere', 'best', 'bonus', 'bulk', 'business', 'buy', 'card', 'cash', 'cd', 'check', 'click', 'com', 'company', 'conference', 'cost', 'credit', 'customer', 'day', 'discussion', 'dollar', 'earn', 'easy', 'edu', 'email', 'english', 'ever', 'every', 'fax', 'financial', 'free', 'fun', 'grammar', 'guarantee', 'here', 'home', 'hour', 'hundred', 'income', 'instruction', 'internet', 'investment', 'language', 'linguist', 'linguistic', 'linguistics', 'list', 'live', 'll', 'mail', 'mailing', 'market', 'million', 'money', 'month', 'name', 'need', 'offer', 'online', 'order', 'our', 'over', 'papers', 'pay', 'product', 'profit', 'program', 'purchase', 'receive', 'remove', 'report', 'sale', 'save', 'sell', 'send', 'service', 'simply', 'speaker', 'start', 'success', 'syntax', 'theory', 'thousand', 'today', 'toll', 'university', 'want', 'watch', 'week', 'win', 'workshop', 'yours', 'yourself', 'zip']\n",
            "Top 1000 features: ['10', '100', '1302', '1341', '149', '1618', '196', '1992', '1993', '1994', '1995', '1998', '1999', '20', '200', '209', '24', '25', '250', '30', '300', '3005', '31', '3d', '400', '470', '50', '500', '55', '550', '600', '700', '800', '888', '90', '95', '99', 'able', 'absolutely', 'abstract', 'ac', 'academic', 'acceptance', 'access', 'accommodation', 'accountant', 'accurately', 'acquisition', 'action', 'ad', 'add', 'additional', 'address', 'addressed', 'addresses', 'adult', 'advantage', 'advertise', 'advertisement', 'advertiser', 'affiliation', 'afford', 'affordable', 'after', 'again', 'ahead', 'aim', 'air', 'allow', 'almost', 'alone', 'already', 'alter', 'alway', 'always', 'am', 'amateur', 'amaze', 'amazing', 'amex', 'among', 'amount', 'analysis', 'announcement', 'anyone', 'anything', 'anytime', 'anywhere', 'aol', 'application', 'apply', 'approach', 'april', 'are', 'argue', 'argument', 'article', 'aspect', 'asset', 'association', 'assume', 'astonishment', 'au', 'august', 'author', 'automatically', 'away', 'awesome', 'back', 'bank', 'bankruptcy', 'batch', 'beach', 'before', 'bel', 'believe', 'believer', 'below', 'benefit', 'benefits', 'berlin', 'best', 'bet', 'better', 'between', 'big', 'bill', 'billboard', 'billion', 'bin', 'biz', 'blast', 'blvd', 'bonus', 'book', 'bottom', 'box', 'boy', 'boyfriend', 'brand', 'break', 'bulk', 'business', 'button', 'buy', 'buyer', 'cable', 'call', 'cambridge', 'camera', 'campaign', 'campus', 'can', 'capital', 'capitalfm', 'car', 'card', 'case', 'cash', 'casino', 'catchy', 'cd', 'cds', 'celebrity', 'cent', 'central', 'centre', 'chair', 'chance', 'chapter', 'charge', 'chat', 'check', 'chinese', 'chomsky', 'choose', 'city', 'classified', 'clause', 'clean', 'cleanest', 'clearance', 'click', 'client', 'cloth', 'club', 'code', 'cognitive', 'colleague', 'collect', 'color', 'com', 'committee', 'communication', 'company', 'comparative', 'competition', 'complete', 'completely', 'complex', 'compliance', 'comply', 'compuserve', 'computational', 'computer', 'conceal', 'concept', 'conference', 'confidential', 'consider', 'consist', 'constraint', 'construction', 'content', 'context', 'continue', 'contribution', 'convenience', 'copy', 'corporation', 'corporations', 'corpus', 'cost', 'countless', 'course', 'cram', 'create', 'credit', 'creditor', 'criminal', 'cross', 'cultural', 'culture', 'customer', 'cut', 'cyber', 'daily', 'datum', 'dave', 'david', 'day', 'days', 'de', 'deadline', 'debt', 'decide', 'define', 'definitely', 'degree', 'delete', 'deliver', 'delivery', 'department', 'deposit', 'dept', 'der', 'description', 'desire', 'desirous', 'development', 'dial', 'dialect', 'dictionary', 'did', 'digital', 'discipline', 'discourse', 'discover', 'discuss', 'discussion', 'dissertation', 'distinction', 'divorce', 'dollar', 'dollars', 'don', 'dori', 'doubt', 'down', 'downline', 'download', 'downpayment', 'dream', 'dupe', 'duplicate', 'dutch', 'each', 'early', 'earn', 'earnings', 'earth', 'easiest', 'easily', 'easy', 'ed', 'editor', 'edu', 'effective', 'effort', 'eliminate', 'else', 'email', 'emailer', 'embark', 'empirical', 'enclose', 'engine', 'english', 'enjoy', 'enter', 'enterprise', 'entertainment', 'entire', 'entrepreneur', 'envelope', 'error', 'esq', 'estate', 'et', 'european', 'evaluating', 'even', 'ever', 'every', 'everyone', 'everythe', 'everything', 'evidence', 'exact', 'exactly', 'exceedingly', 'excess', 'excite', 'exclusive', 'expensive', 'expiration', 'expression', 'extra', 'extractor', 'extraordinary', 'extremely', 'ez', 'fabulous', 'faculty', 'fairchild', 'faith', 'family', 'fantastic', 'fast', 'faster', 'fastest', 'fax', 'february', 'federal', 'few', 'figure', 'file', 'fill', 'filled', 'filter', 'financial', 'financially', 'first', 'fl', 'flamer', 'fm', 'focus', 'follow', 'forever', 'forget', 'formal', 'fortune', 'forum', 'four', 'fraction', 'framework', 'france', 'free', 'freedom', 'french', 'fresh', 'friend', 'fun', 'future', 'gamble', 'game', 'general', 'generate', 'generative', 'genie', 'german', 'germany', 'gift', 'girl', 'girlfriend', 'goal', 'gold', 'golden', 'goods', 'gov', 'graduate', 'grammar', 'grammatical', 'great', 'greatest', 'grow', 'grumble', 'guarantee', 'guaranteed', 'guide', 'hand', 'happen', 'happy', 'hardcore', 'header', 'hello', 'help', 'here', 'hesitate', 'hi', 'high', 'historical', 'hit', 'hobby', 'hold', 'holiday', 'home', 'honest', 'hot', 'hotmail', 'hottest', 'hour', 'hours', 'hr', 'http', 'huge', 'human', 'hundred', 'hundreds', 'id', 'ii', 'illegal', 'imagine', 'immediate', 'immediately', 'implication', 'important', 'included', 'income', 'increase', 'incredible', 'industry', 'inexpensive', 'inflation', 'info', 'information', 'instant', 'instantly', 'institute', 'instruct', 'instructed', 'instruction', 'instructions', 'insurance', 'interaction', 'internet', 'interpretation', 'introduction', 'intrusion', 'invest', 'investment', 'invite', 'is', 'isbn', 'isp', 'issue', 'jackson', 'jame', 'japan', 'japanese', 'john', 'join', 'journal', 'jump', 'june', 'junk', 'juno', 'keep', 'kid', 'language', 'lanse', 'latest', 'laugh', 'law', 'lawful', 'least', 'leave', 'lecture', 'legal', 'legally', 'legitimate', 'length', 'less', 'let', 'letter', 'level', 'lexical', 'lexicon', 'life', 'lifetime', 'limited', 'line', 'ling', 'linguist', 'linguistic', 'linguistics', 'link', 'list', 'lists', 'literature', 'little', 'live', 'living', 'll', 'loan', 'logic', 'lose', 'lot', 'lottery', 'love', 'low', 'luck', 'lucky', 'magazine', 'mail', 'mailbox', 'mailer', 'mailing', 'make', 'making', 'many', 'march', 'mark', 'market', 'marketer', 'marketing', 'mastercard', 'mci', 'mclaughlin', 'md', 'meg', 'mega', 'member', 'merciless', 'message', 'method', 'methodology', 'michael', 'million', 'millionaire', 'millions', 'miss', 'mit', 'mlm', 'model', 'modem', 'modern', 'moment', 'money', 'month', 'monthly', 'morphological', 'morphology', 'mortgage', 'most', 'move', 'movie', 'msn', 'much', 'multi', 'multus', 'murkowskus', 'music', 'must', 'name', 'native', 'natural', 'nc', 'need', 'net', 'netherland', 'never', 'news', 'newsgroup', 'next', 'nl', 'nothing', 'notification', 'notion', 'noun', 'number', 'numbers', 'object', 'obligation', 'off', 'offer', 'office', 'once', 'one', 'online', 'operate', 'opportunity', 'optional', 'order', 'ordering', 'orders', 'organize', 'organizer', 'our', 'ours', 'over', 'overflow', 'overload', 'overnight', 'owe', 'own', 'owner', 'oxford', 'package', 'page', 'paper', 'papers', 'paradise', 'parallel', 'participate', 'particular', 'particularly', 'partner', 'pass', 'password', 'paste', 'pattern', 'pay', 'payable', 'payment', 'pc', 'penny', 'per', 'percentage', 'perfectly', 'persistent', 'personal', 'perspective', 'phone', 'phonetic', 'phonetics', 'phonological', 'phonology', 'phrase', 'pic', 'pick', 'piece', 'pile', 'place', 'plan', 'plans', 'please', 'plus', 'poorer', 'population', 'porn', 'post', 'postage', 'postal', 'postscript', 'potential', 'powerful', 'pp', 'practically', 'pragmatic', 'preliminary', 'premium', 'prepared', 'present', 'presentation', 'price', 'principle', 'print', 'privacy', 'proceeding', 'proceedings', 'process', 'prodigy', 'product', 'prof', 'profanity', 'professional', 'professor', 'profit', 'profitable', 'program', 'programme', 'programs', 'project', 'promotion', 'prompt', 'proof', 'proposal', 'protect', 'protection', 'proud', 'prove', 'provider', 'psycholinguistic', 'publication', 'publish', 'purchase', 'put', 'qualify', 'query', 'quick', 'quickly', 'quit', 'race', 'radio', 'raleigh', 'ram', 'rat', 'rate', 'rather', 're', 'reach', 'read', 'real', 'realistic', 'really', 'reap', 'receive', 'recent', 'recession', 'record', 'recruit', 'reference', 'refinance', 'refund', 'reg', 'registration', 'relate', 'relation', 'relax', 'release', 'released', 'relevant', 'remember', 'removal', 'remove', 'removed', 'reply', 'report', 'reports', 'representation', 'request', 'require', 'research', 'researcher', 'resell', 'residual', 'response', 'retail', 'retire', 'retirement', 'return', 'revenue', 'review', 'rich', 'richer', 'right', 'rights', 'rip', 'risk', 'robbery', 'robert', 'rockland', 'role', 'roll', 'rom', 'run', 'rush', 'sake', 'sale', 'sales', 'same', 'satisfy', 'save', 'savings', 'scam', 'scholar', 'school', 'science', 'scientific', 'search', 'second', 'secret', 'secrets', 'section', 'secure', 'security', 'sell', 'selle', 'seller', 'semantic', 'semantics', 'send', 'sender', 'sent', 'sentence', 'september', 'server', 'service', 'services', 'session', 'seven', 'several', 'sex', 'sexual', 'sexually', 'ship', 'shock', 'shop', 'show', 'signature', 'simple', 'simply', 'sincerely', 'sit', 'site', 'size', 'skeptical', 'small', 'society', 'sociolinguistic', 'software', 'someday', 'someone', 'soon', 'sources', 'spam', 'spanish', 'speak', 'speaker', 'speech', 'speed', 'spend', 'spokane', 'sponsor', 'spout', 'staggering', 'stamped', 'start', 'started', 'state', 'stealth', 'step', 'stock', 'stop', 'store', 'structure', 'student', 'study', 'stun', 'style', 'submission', 'submit', 'succeed', 'success', 'successful', 'suite', 'summary', 'super', 'sure', 'symposium', 'syntactic', 'syntax', 'tax', 'teacher', 'teen', 'tel', 'tell', 'term', 'test', 'testimonial', 'text', 'thank', 'theme', 'theoretical', 'theory', 'thing', 'those', 'thousand', 'thousands', 'through', 'ticket', 'tip', 'tips', 'today', 'toll', 'too', 'top', 'topic', 'total', 'totally', 'totals', 'toy', 'trade', 'translation', 'trash', 'tremendous', 'trial', 'true', 'truly', 'trust', 'try', 'turn', 'tv', 'two', 'type', 'typology', 'uk', 'undeliverable', 'under', 'uni', 'unique', 'univ', 'university', 'unlimit', 'unlimited', 'unproductive', 'unsolicit', 'unsubscribe', 'until', 'upgrade', 'us', 'using', 'utility', 'vacation', 'valuable', 'van', 'vanish', 'variation', 'variety', 've', 'verb', 'verify', 'video', 'virtual', 'visa', 'visit', 'vium', 'volume', 'vowel', 'vulgarity', 'wait', 'wall', 'want', 'waste', 'watch', 'wealth', 'web', 'webmaster', 'week', 'weekly', 'weeks', 'welcome', 'whatsoever', 'why', 'wilburn', 'win', 'win95', 'window', 'winner', 'wisely', 'wish', 'within', 'woman', 'word', 'work', 'works', 'workshop', 'world', 'worldwide', 'worth', 'wrap', 'write', 'xxx', 'yahoo', 'ye', 'yes', 'yours', 'yourself', 'zip']\n"
          ]
        }
      ],
      "source": [
        "# Vectorize the emails\n",
        "tfidf_vectorizer = CountVectorizer()\n",
        "X = tfidf_vectorizer.fit_transform(train_emails)\n",
        "\n",
        "# Perform feature selection with mutual_info_classif\n",
        "for N in [10, 100, 1000]:\n",
        "    selector = SelectKBest(mutual_info_classif, k=N)\n",
        "    X_new = selector.fit_transform(X, train_labels)\n",
        "\n",
        "    # Get the feature names\n",
        "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "    selected_features = [feature_names[i] for i in selector.get_support(indices=True)]\n",
        "    print(f\"Top {N} features: {selected_features}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqw_redJSJ09"
      },
      "source": [
        "# 3) Implementing Classifiers\n",
        "implementing the following classifiers:\n",
        "\n",
        "○\tBernoulli NB classifier with binary features;\n",
        "\n",
        "○\tMultinomial NB with binary features; and\n",
        "\n",
        "○\tMultinomial NB with term frequency (TF) features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCIZbGpmqLoK",
        "outputId": "f15987c2-8220-451f-e71e-6d55f55b79e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier: BernoulliNB\n",
            "Top 10 features:\n",
            "Precision = 0.8846153846153846, Recall = 0.3129251700680272, Latency = 124.10886311531067 seconds\n",
            "Top 100 features:\n",
            "Precision = 0.9565217391304348, Recall = 0.29931972789115646, Latency = 119.53114986419678 seconds\n",
            "Top 1000 features:\n",
            "Precision = 1.0, Recall = 0.30612244897959184, Latency = 119.71840333938599 seconds\n",
            "Classifier: MultinomialNB_binary\n",
            "Top 10 features:\n",
            "Precision = 0.8846153846153846, Recall = 0.3129251700680272, Latency = 121.85881352424622 seconds\n",
            "Top 100 features:\n",
            "Precision = 0.9811320754716981, Recall = 0.35374149659863946, Latency = 120.03866958618164 seconds\n",
            "Top 1000 features:\n",
            "Precision = 1.0, Recall = 0.46258503401360546, Latency = 124.37408089637756 seconds\n",
            "Classifier: MultinomialNB_tf\n",
            "Top 10 features:\n",
            "Precision = 0.8666666666666667, Recall = 0.35374149659863946, Latency = 123.65640473365784 seconds\n",
            "Top 100 features:\n",
            "Precision = 0.95, Recall = 0.3877551020408163, Latency = 122.05910515785217 seconds\n",
            "Top 1000 features:\n",
            "Precision = 1.0, Recall = 0.46258503401360546, Latency = 124.18814325332642 seconds\n"
          ]
        }
      ],
      "source": [
        "# Vectorize the emails\n",
        "vectorizer = CountVectorizer(binary = True)\n",
        "X_train = vectorizer.fit_transform(train_emails)\n",
        "X_test = vectorizer.transform(test_emails)\n",
        "\n",
        "# Define the classifiers\n",
        "classifiers = {\n",
        "    \"BernoulliNB\": BernoulliNB(),\n",
        "    \"MultinomialNB_binary\": MultinomialNB(),\n",
        "    \"MultinomialNB_tf\": MultinomialNB()\n",
        "}\n",
        "\n",
        "# Vectorize the emails with term frequency features\n",
        "vectorizer_tf = CountVectorizer()\n",
        "X_train_tf = vectorizer_tf.fit_transform(train_emails)\n",
        "X_test_tf = vectorizer_tf.transform(test_emails)\n",
        "\n",
        "# For each classifier and for N = {10, 100, 1000}\n",
        "for classifier_name, classifier in classifiers.items():\n",
        "    print(f\"Classifier: {classifier_name}\")\n",
        "    if classifier_name == \"MultinomialNB_tf\":\n",
        "        X_train = X_train_tf\n",
        "        X_test = X_test_tf\n",
        "    # Perform cross-validation and calculate spam precision and spam recall\n",
        "    for N in [10, 100, 1000]:\n",
        "        start_time = time.time()\n",
        "        selector = SelectKBest(mutual_info_classif, k=N)\n",
        "        X_train_new = selector.fit_transform(X_train, train_labels)\n",
        "        X_test_new = selector.transform(X_test)\n",
        "        classifier.fit(X_train_new, train_labels)\n",
        "        y_pred = classifier.predict(X_test_new)\n",
        "        precision = precision_score(test_labels, y_pred, pos_label=1)\n",
        "        recall = recall_score(test_labels, y_pred, pos_label=1)\n",
        "        latency = time.time() - start_time\n",
        "        # Get the feature names\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        selected_features = [feature_names[i] for i in selector.get_support(indices=True)]\n",
        "        print(f\"Top {N} features:\")\n",
        "        print(f\"Precision = {precision}, Recall = {recall}, Latency = {latency} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iERbvLmcSW6H"
      },
      "source": [
        "# SVM Spam Filter\n",
        "Designing a Support Vector Machine (SVM) based spam filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Wi9G1XmsGBx",
        "outputId": "9b3a738a-d732-4b6c-ef18-9bf2e49e3022"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM Classifier: Precision = 0.9861111111111112, Recall = 0.48299319727891155, Latency = 220.54181170463562 seconds\n",
            "Best parameters: {'C': 10, 'gamma': 1}\n"
          ]
        }
      ],
      "source": [
        "# Vectorize the emails with TF-IDF features\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train = vectorizer.fit_transform(train_emails)\n",
        "X_test = vectorizer.transform(test_emails)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001]}\n",
        "\n",
        "# Initialize the SVM classifier\n",
        "svm = SVC(kernel='linear')  # Linear Kernel\n",
        "\n",
        "# Use GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "start_time = time.time()\n",
        "grid_search.fit(X_train, train_labels)\n",
        "\n",
        "# Get the best parameters and the best estimator\n",
        "best_params = grid_search.best_params_\n",
        "best_svm = grid_search.best_estimator_\n",
        "\n",
        "# Predict the test set results\n",
        "y_pred = best_svm.predict(X_test)\n",
        "\n",
        "# Calculate precision and recall\n",
        "precision = precision_score(test_labels, y_pred, pos_label=1)\n",
        "recall = recall_score(test_labels, y_pred, pos_label=1)\n",
        "latency = time.time() - start_time\n",
        "\n",
        "# Print precision, recall and latency\n",
        "print(f\"SVM Classifier: Precision = {precision}, Recall = {recall}, Latency = {latency} seconds\")\n",
        "print(f\"Best parameters: {best_params}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAvBQKGGRdqu"
      },
      "source": [
        "Methodology :\n",
        "\n",
        "To begin, text data undergoes a transformation into TF-IDF feature vectors, assessing the significance of words within the documents.\n",
        "\n",
        "Following this, a grid search is employed for hyperparameter tuning to enhance the SVM classifier. This tuning process primarily focuses on optimizing the 'C' regularization parameter and the 'gamma' kernel parameter. The ultimate goal is to maximize classification accuracy using a 5-fold cross-validation technique. Initially, the SVM classifier employs a linear kernel.\n",
        "\n",
        "Once the hyperparameter tuning process is complete, the best hyperparameters are determined, along with the corresponding SVM estimator. Subsequently, the trained SVM model is utilized to make predictions for the test dataset, and precision and recall scores are computed to assess its performance.\n",
        "\n",
        "Ultimately, the results, encompassing precision, recall, and training duration, are displayed in the console alongside the best hyperparameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOz-0FXSRf9K"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
